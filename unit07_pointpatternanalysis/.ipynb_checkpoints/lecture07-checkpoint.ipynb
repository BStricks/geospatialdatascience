{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"frontmatter text-center\">\n",
    "<h1>Geospatial Data Science</h1>\n",
    "<h2>Lecture 7: Point pattern analysis</h2>\n",
    "<h3>IT University of Copenhagen, Spring 2022</h3>\n",
    "<h3>Instructor: Michael Szell</h3>\n",
    "</div>\n",
    "\n",
    "# Source\n",
    "This notebook was adapted from:\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bH/lab_H.html\n",
    "* Geographic Data Science with Python: https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Points are spatial entities that can be understood in two fundamentally different ways. On the one hand, points can be seen as fixed objects in space, which is to say their location is taken as given (*exogenous*). In this case, analysis of points is very similar to that of other types of spatial data such as polygons and lines. On the other hand, points can be seen as the occurence of an event that could theoretically take place anywhere but only manifests in certain locations. This is the approach we will adopt in the rest of the notebook.\n",
    "\n",
    "When points are seen as events that could take place in several locations but only happen in a few of them, a collection of such events is called a *point pattern*. In this case, the location of points is one of the key aspects of interest for analysis. A good example of a point pattern is crime events in a city: they could technically happen in many locations but we usually find crimes are committed only in a handful of them. Point patterns can be *marked*, if more attributes are provided with the location, or *unmarked*, if only the coordinates of where the event occured are provided. Continuing the crime example, an unmarked pattern would result if only the location where crimes were committed was used for analysis, while we would be speaking of a marked point pattern if other attributes, such as the type of crime, the extent of the damage, etc. was provided with the location.\n",
    "\n",
    "Point pattern analysis is thus concerned with the description, statistical characerization, and modeling of point patterns, focusing specially on the generating process that gives rise and explains the observed data. *What's the nature of the distribution of points?* *Is there any structure we can statistically discern in the way locations are arranged over space?* *Why do events occur in those places and not in others?* These are all questions that point pattern analysis is concerned with.\n",
    "\n",
    "This notebook aims to be a gentle introduction to working with point patterns in Python. As such, it covers how to read, process and transform point data, as well as several common ways to visualize point patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from ipywidgets import interact, fixed\n",
    "from pointpats import distance_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photographs\n",
    "\n",
    "\n",
    "We are going to dip our toes in the lake of point data by looking at a sample of geo-referenced photographs in Tokyo. The dataset comes from the GDS Book https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html and contains photographs voluntarily uploaded to the Flickr service.\n",
    "\n",
    "Let's read the dataset first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read remote file\n",
    "tokyo = pd.read_csv(\"files/tokyo_clean.csv\")\n",
    "tokyo.info()\n",
    "tokyo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Administrative areas\n",
    "\n",
    "We will later use administrative areas for aggregation. Let's load them upfront first. These are provided with the course and available online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file in\n",
    "areas = gpd.read_file(\"files/tokyo_admin_boundaries.geojson\")\n",
    "areas.info()\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final bit we need to get out of the way is attaching the administrative area code where a photo is located to each area. This can be done with a GIS operation called \"spatial join\", see also: https://geopandas.org/en/stable/docs/user_guide/mergingdata.html#spatial-joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a geodataframe out of the dataframe\n",
    "tokyo_gdf = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"geometry\": gpd.points_from_xy(\n",
    "            tokyo[\"longitude\"], tokyo[\"latitude\"]\n",
    "        )\n",
    "    }, crs=\"EPSG:4326\"\n",
    ").join(tokyo)\n",
    "# Create the spatially joined data: photos + admin areas\n",
    "crosswalk = gpd.sjoin(tokyo_gdf, areas, how=\"inner\")\n",
    "# Add the admin area code to the original dataframe\n",
    "tokyo[\"admin_area\"] = crosswalk[\"GID_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokyo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of a Point Pattern\n",
    "\n",
    "We will spend the rest of this notebook learning different ways to visualize a point pattern. In particular, we will consider to main strategies: one relies on aggregating the points into polygons, while the second one is based on creating continuous surfaces using kernel density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one\n",
    "\n",
    "The first approach we review here is the one-to-one approach, where we place a dot on the screen for every point to visualise. In Python, one way to do this is with the `scatter` method in the Pandas visualisation layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a dot for every image\n",
    "tokyo.plot.scatter(\"longitude\", \"latitude\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this does not give us much geographical context and, since there are many points, it is hard to see any pattern in areas of high density. Let's tweak the dot display and add a basemap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot photographs with smaller, more translucent dots\n",
    "ax = tokyo.plot.scatter(\n",
    "    \"longitude\", \n",
    "    \"latitude\",\n",
    "    s=0.25,\n",
    "    c=\"xkcd:bright yellow\",\n",
    "    alpha=0.5,\n",
    "    figsize=(9, 9)\n",
    ")\n",
    "# remove axis\n",
    "ax.set_axis_off()\n",
    "# Add dark basemap\n",
    "cx.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.DarkMatter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use jointplot to plot the points together with their lat and lon distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_axes = sns.jointplot(\n",
    "    x='longitude', y='latitude', data=tokyo, s=0.5\n",
    ")\n",
    "cx.add_basemap(\n",
    "    joint_axes.ax_joint,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.PositronNoLabels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Points meet polygons\n",
    "\n",
    "The approach presented above works until a certain number of points to plot; tweaking dot transparency and size only gets us so far and, at some point, we need to shift the focus. Having learned about visualizing lattice (polygon) data, an option is to \"turn\" points into polygons and apply techniques like choropleth mapping to visualize their spatial distribution. To do that, we will overlay a polygon layer on top of the point pattern, *join* the points to the polygons by assigning to each point the polygon where they fall into, and create a choropleth of the counts by polygon. \n",
    "\n",
    "This approach is intuitive but of course raises the following question: *what polygons do we use to aggregate the points?* Ideally, we want a boundary delineation that matches as closely as possible the point generating process and partitions the space into areas with a similar internal intensity of points. However, that is usually not the case, no less because one of the main reasons we typically want to visualize the point pattern is to learn about such generating process, so we would typically not know a priori whether a set of polygons match it. If we cannot count on the ideal set of polygons to begin with, we can adopt two more realistic approaches: using a set of pre-existing irregular areas or create a artificial set of regular polygons. Let's explore both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irregular lattices\n",
    "\n",
    "To exemplify this approach, we will use the administrative areas we have loaded above. Let's add them to the figure above to get better context (unfold the code if you are interested in seeing exactly how we do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot photographs with smaller, more translucent dots\n",
    "ax = tokyo.plot.scatter(\n",
    "    \"longitude\", \n",
    "    \"latitude\",\n",
    "    s=0.25,\n",
    "    c=\"xkcd:bright yellow\",\n",
    "    alpha=0.5,\n",
    "    figsize=(9, 9)\n",
    ")\n",
    "# Add administrative boundaries\n",
    "areas.plot(\n",
    "    ax=ax,\n",
    "    facecolor=\"none\",\n",
    "    edgecolor=\"xkcd:pale lavender\"\n",
    ")\n",
    "# remove axis\n",
    "ax.set_axis_off()\n",
    "# Add dark basemap\n",
    "cx.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.DarkMatter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to know how many photographs each are contains. Our photograph table already contains the area ID, so all we need to do here is counting by area and attaching the count to the `areas` table. We rely here on the `groupby` operator which takes all the photos in the table and \"groups\" them \"by\" their administrative ID. Once grouped, we apply the method `size`, which counts how many elements each group has and returns a column indexed on the LSOA code with all the counts as its values. We end by assigning the counts to a newly created column in the `areas` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counts\n",
    "photos_by_area = tokyo.groupby(\"admin_area\").size()\n",
    "# Assign counts into a table of its own \n",
    "# and joins it to the areas table\n",
    "areas = areas.join(\n",
    "    pd.DataFrame({\"photo_count\": photos_by_area}),\n",
    "    on=\"GID_2\"\n",
    ")\n",
    "areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines above have created a new column in our `areas` table that contains the number of photos that have been taken within each of the polygons in the table.\n",
    "\n",
    "At this point, we are ready to map the counts. Technically speaking, this is a choropleth just as we have seen many times before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot the equal interval choropleth and add a legend\n",
    "areas.plot(\n",
    "    column='photo_count', \n",
    "    scheme='quantiles', \n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    legend_kwds={\"loc\": 4}\n",
    ")\n",
    "# Remove the axes\n",
    "ax.set_axis_off()\n",
    "# Set the title\n",
    "ax.set_title(\"Quantile map of photo counts by administrative boundary\")\n",
    "# Add dark basemap\n",
    "cx.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.DarkMatterNoLabels\n",
    ")\n",
    "# Draw map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map above clearly shows a concentration of photos in the centre of Tokyo. However, it is important to remember that the map is showing *raw* counts of tweets. In the case to photos, as with many other phenomena, it is crucial to keep in mind the \"container geography\". In this case, different administrative areas have different sizes. Everything else equal, a larger polygon may contain more photos, simply because it covers a larger space. To obtain a more accurate picture of the _intensity_ of photos by area, what we would like to see is a map of the *density* of photos, not of raw counts. To do this, we can divide the count per polygon by the area of the polygon.\n",
    "\n",
    "Let's first calculate the area in Sq. metres of each administrative delineation.\n",
    "\n",
    "First we need to convert our polygons into a projected CRS. We use the [Japan Plane Rectangular CS XVII system](http://epsg.io/2459). Also, we multiply the area by `1e-6` to express the area in squared Km instead of sq. metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas[\"area_sqm\"] = areas.to_crs(epsg=2459).area * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can add the photo density as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas[\"photo_density\"] = areas[\"photo_count\"] / areas[\"area_sqm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the density at hand, creating the new choropleth is similar as above (check the code in the expandable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot the equal interval choropleth and add a legend\n",
    "areas.plot(\n",
    "    column='photo_density', \n",
    "    scheme='quantiles', \n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    legend_kwds={\"loc\": 4}\n",
    ")\n",
    "# Remove the axes\n",
    "ax.set_axis_off()\n",
    "# Set the title\n",
    "ax.set_title(\"Quantile map of photo density by administrative boundary\")\n",
    "# Add dark basemap\n",
    "cx.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.DarkMatterNoLabels\n",
    ")\n",
    "# Draw map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern in the raw counts is similar to that of density, but we can see how some peripheral, large areas are \"downgraded\" when correcting for their size, while some smaller polygons in the centre display a higher value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular lattices: hex-binning\n",
    "\n",
    "Sometimes we either do not have any polygon layer to use or the ones we have are not particularly well suited to aggregate points into them. In these cases, a sensible alternative is to create an artificial topology of polygons that we can use to aggregate points. There are several ways to do this but the most common one is to create a grid of hexagons. This provides a regular topology (every polygon is of the same size and shape) that, unlike circles, cleanly exhausts all the space without overlaps and has more edges than squares, which alleviates edge problems.\n",
    "\n",
    "Python has a simplified way to create this hexagon layer *and* aggregate points into it in one shot thanks to the method `hexbin`, which is available in every axis object (e.g. `ax`). Let us first see how you could create a map of the hexagon layer alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Add hexagon layer that displays count of points in each polygon\n",
    "hb = ax.hexbin(\n",
    "    tokyo[\"longitude\"], \n",
    "    tokyo[\"latitude\"], \n",
    "    gridsize=50, \n",
    ")\n",
    "# Add a colorbar (optional)\n",
    "plt.colorbar(hb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how all it takes is to set up the figure and call `hexbin` directly using the set of coordinate columns (`tokyo[\"longitude\"]` and `tokyo[\"latitude\"]`). Additional arguments we include is the number of hexagons by axis (`gridsize`, 50 for a 50 by 50 layer), and the transparency we want (80%). Additionally, we include a colorbar to get a sense of how counts are mapped to colors. Note that we need to pass the name of the object that includes the `hexbin` (`hb` in our case), but keep in mind this is optional, you do not need to always create one.\n",
    "\n",
    "Once we know the basics, we can dress it up a bit more for better results (expand to see code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(16, 12))\n",
    "# Add hexagon layer that displays count of points in each polygon\n",
    "hb = ax.hexbin(\n",
    "    tokyo[\"longitude\"], \n",
    "    tokyo[\"latitude\"], \n",
    "    gridsize=50,\n",
    "    alpha=0.5,\n",
    "    edgecolor=\"none\"\n",
    ")\n",
    "# Add a colorbar (optional)\n",
    "plt.colorbar(hb)\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add basemap\n",
    "cx.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.DarkMatterNoLabels\n",
    ")\n",
    "# Add title of the map\n",
    "ax.set_title(\"Hex-binning of photos in Tokyo\")\n",
    "# Draw map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Density Estimation\n",
    "\n",
    "\n",
    "Using a hexagonal binning can be a quick solution when we do not have a good polygon layer to overlay the points directly and some of its properties, such as the equal size of each polygon, can help alleviate some of the problems with a \"bad\" irregular topology (one that does not fit the underlying point generating process). However, it does not get around the issue of the modifiable areal unit problem (MAUP): at the end of the day, we are still imposing arbitrary boundary lines and aggregating based on them, so the possibility of mismatch with the underlying distribution of the point pattern is very real.\n",
    "\n",
    "One way to work around this problem is to avoid aggregating into another geography altogether. Instead, we can aim at estimating the *continuous* observed probability distribution. The most commonly used method to do this is the so called *kernel density estimate* (KDE). The idea behind KDEs is to count the number of points in a *continuous* way. Instead of using discrete counting, where you include a point in the count if it is inside a certain boundary and ignore it otherwise, KDEs use functions (kernels) that include points but give different weights to each one depending of how far of the location where we are counting the point is.\n",
    "\n",
    "The actual algorithm to estimate a kernel density is not trivial but its application in Python is rather simplified by the use of Seaborn. KDE's however are fairly computationally intensive. When you have a large point pattern like we do in the `tokyo` example (10,000 points), its computation can take a bit long. To get around this issue, we create a random subset, which retains the overall structure of the pattern, but with much fewer points. Let's take a subset of 1,000 random points from our original table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random subset of 1,000 rows from `tokyo`\n",
    "tokyo_sub = tokyo.sample(1000, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we need to specify the size of the resulting subset (1,000), and we also add a value for `random_state`; this ensures that the sample is always the same and results are thus reproducible.\n",
    "\n",
    "Same as above, let us first see how to create a quick KDE. For this we rely on Seaborn's `kdeplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    tokyo_sub[\"longitude\"], \n",
    "    tokyo_sub[\"latitude\"], \n",
    "    n_levels=10, \n",
    "    shade=True, \n",
    "    cmap='BuPu'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn greatly streamlines the process and boils it down to a single line. The method `sns.kdeplot` (which we can also use to create a KDE of a single variable) takes the X and Y coordinate of the points as the only compulsory attributes. In addition, we specify the number of levels we want the color gradient to have (`n_levels`), whether we want to color the space in between each level (`shade`, yes), and the colormap of choice.\n",
    "\n",
    "Once we know how the basic logic works, we can insert it into the usual mapping machinery to create a more complete plot. The main difference here is that we now have to tell `sns.kdeplot` where we want the surface to be added (`ax` in this case). Toggle the expandable to find out the code that produces the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(12, 12))\n",
    "# Add KDE layer that displays probability surface\n",
    "sns.kdeplot(\n",
    "    tokyo_sub[\"longitude\"], \n",
    "    tokyo_sub[\"latitude\"], \n",
    "    n_levels=50, \n",
    "    shade=True,\n",
    "    alpha=0.25,\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add basemap\n",
    "cx.add_basemap(\n",
    "    ax, \n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.DarkMatterNoLabels\n",
    ")\n",
    "# Add title of the map\n",
    "ax.set_title(\"KDE of photos in Tokyo\")\n",
    "# Draw map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate KDE plot with contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_axes = sns.jointplot(\n",
    "    x='longitude', y='latitude', data=tokyo_sub, kind=\"kde\"\n",
    ")\n",
    "cx.add_basemap(\n",
    "    joint_axes.ax_joint,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=cx.providers.CartoDB.PositronNoLabels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical point pattern analysis with Ripley's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = tokyo_sub[['x','y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripley's functions focus on the distributions of two quantities in a point pattern: nearest neighbor distances and what we will term \"gaps\" in the pattern. Each of these characterize an aspect of the point pattern as we increase the distance range from each point to calculate them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ripley's $G$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripley's $G$, focuses on the distribution of nearest neighbor distances. That is, the $G$ function summarizes the distances between each point in the pattern and their nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripley's $G$ keeps track of the proportion of points for which the nearest neighbor is within a given distance threshold, and plots that cumulative percentage against the increasing distance radii. The distribution of these cumulative percentage has a distinctive shape under completely spatially random processes. The intuition behind Ripley's G goes as follows: we can learn about how similar our pattern is to a spatially random one by computing the cumulative distribution of nearest neighbor distances over increasing distance thresholds, and comparing it to that of a set of simulated patterns that follow a known spatially-random process. Usually, a homogeneous spatial Poisson point process is used as such a reference distribution, also called *complete spatial randomness* (CSR): https://en.wikipedia.org/wiki/Complete_spatial_randomness\n",
    "\n",
    "To do this in the `pointpats` package, we can use the `g_test` function, which computes both the `G` function for the empirical data *and* these hypothetical replications under a completely spatially random process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g_test = distance_statistics.g_test(\n",
    "    coordinates, support=40, keep_simulations=True, n_simulations=999\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about these distributions of distances, a \"clustered\" pattern must have more points near one another than a pattern that is \"dispersed\"; and a completely random pattern should have something in between. Therefore, if the $G$ function increases *rapidly* with distance, we probably have a clustered pattern. If it increases *slowly* with distance, we have a dispersed pattern. Something in the middle will be difficult to distinguish from pure chance.\n",
    "\n",
    "We can visualize this below. On the left, we plot the $G(d)$ function, with distance-to-point ($d$) on the horizontal axis and the fraction of nearest neighbor distances smaller than $d$ on the right axis. In red, the empirical cumulative distribution of nearest neighbor distances is shown. In cyan, simulations (like the `random` pattern shown in the previous section) are shown. The bright blue line represents the average of all simulations, and the darker blue/black band around it represents the middle 95% of simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(9,3), \n",
    "                    gridspec_kw=dict(width_ratios=(6,3)))\n",
    "# plot all the simulations with very fine lines\n",
    "ax[0].plot(g_test.support, g_test.simulations.T, color='k', alpha=.01)\n",
    "# and show the average of simulations\n",
    "ax[0].plot(g_test.support, np.median(g_test.simulations, axis=0), color='cyan', \n",
    "         label='median simulation')\n",
    "\n",
    "\n",
    "# and the observed pattern's G function\n",
    "ax[0].plot(g_test.support, g_test.statistic, label = 'observed', color='red')\n",
    "\n",
    "# clean up labels and axes\n",
    "ax[0].set_xlabel('distance')\n",
    "ax[0].set_ylabel('% of nearest neighbor\\ndistances shorter')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlim(0,2000)\n",
    "ax[0].set_title(r\"Ripley's $G(d)$ function\")\n",
    "\n",
    "# plot the pattern itself on the next frame\n",
    "ax[1].scatter(*coordinates.T)\n",
    "\n",
    "# and clean up labels and axes there, too\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_title('Pattern')\n",
    "f.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, we see that the red empirical function rises much faster than simulated completely spatially random patterns. This means that the observed pattern of this user's Flickr photographs are *closer* to their nearest neighbors than would be expected from a completely spatially random pattern. The pattern is *clustered.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ripley's $F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function we introduce is Ripley's $F$. Where the $G$ function works by analyzing the distance *between* points in the pattern, the *F* function works by analyzing the distance *to* points in the pattern from locations in empty space. That is why the $F$ function is called the \"the empty space function\", since it characterizes the typical distance from arbitrary points in empty space to the point pattern. More explicitly, the $F$ accumulates, for a growing distance range, the percentage of points that can be found within that range from a random point pattern generated within the extent of the observed pattern. If the pattern has large gaps or empty areas, the $F$ function will increase slowly. But, if the pattern is highly dispersed, then the $F$ function will increase rapidly. The shape of this cumulative distribution is then compared to those constructed by calculating the same cumulative distribution between the random pattern and an additional, random one generated in each simulation step.\n",
    "\n",
    "We can use similar tooling to investigate the $F$ function, since it is so mathematically similar to the $G$ function. This is implemented identically using the `f_test` function in `pointpats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = distance_statistics.f_test(\n",
    "    coordinates, support=40, keep_simulations=True, n_simulations=999\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(\n",
    "    1,2,figsize=(9,3), gridspec_kw=dict(width_ratios=(6,3))\n",
    ")\n",
    "\n",
    "# plot all the simulations with very fine lines\n",
    "ax[0].plot(f_test.support, f_test.simulations.T, color='k', alpha=.01)\n",
    "# and show the average of simulations\n",
    "ax[0].plot(f_test.support, np.median(f_test.simulations, axis=0), color='cyan', \n",
    "         label='median simulation')\n",
    "\n",
    "\n",
    "# and the observed pattern's F function\n",
    "ax[0].plot(f_test.support, f_test.statistic, label = 'observed', color='red')\n",
    "\n",
    "# clean up labels and axes\n",
    "ax[0].set_xlabel('distance')\n",
    "ax[0].set_ylabel('% of nearest point in pattern\\ndistances shorter')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlim(0,2000)\n",
    "ax[0].set_title(r\"Ripley's $F(d)$ function\")\n",
    "\n",
    "# plot the pattern itself on the next frame\n",
    "ax[1].scatter(*coordinates.T)\n",
    "\n",
    "# and clean up labels and axes there, too\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_title('Pattern')\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the $F$ function estimated for the observed pattern increases *much* more slowly than the $F$ functions for the simulated patterns, we can be confident that there are many gaps in our pattern; i.e. the pattern is *clustered*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripley's \"alphabet\" extends to several other letter-named functions that can be used for conducting point pattern analysis in this vein. Good \"next steps\" in your point pattern analysis journey are the extended source chapter for this notebook (https://geographicdata.science/book/notebooks/08_point_pattern_analysis.html) and the `pointpats` documentation for guidance on how to run these in Python: https://pointpats.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters of points\n",
    "\n",
    "In this final section, we will learn a method to identify clusters of points, based on their density across space. To do this, we will use the widely used `DBSCAN` algorithm. For this method, a cluster is a concentration of at least `m` points, each of them within a distance of `r` of at least another point in the cluster. Points in the dataset are then divided into three categories:\n",
    "\n",
    "* *Noise*, for those points outside a cluster.\n",
    "* *Cores*, for those points inside a cluster whith at least `m` points in the cluster within distance `r`.\n",
    "* *Borders* for points inside a cluster with less than `m` other points in the cluster within distance `r`.\n",
    "\n",
    "Both `m` and `r` need to be prespecified by the user before running `DBSCAN`. This is a critical point, as their value can influence significantly the final result. Before exploring this in greater depth, let us get a first run at computing `DBSCAN` in Python.\n",
    "\n",
    "### Basics\n",
    "\n",
    "The heavy lifting is done by the method `DBSCAN`, part of the excellent machine learning library `scikit-learn`. We first set up the details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up algorithm\n",
    "algo = DBSCAN(eps=100, min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "margin"
    ]
   },
   "outputs": [],
   "source": [
    "## Express points in metres\n",
    "# Convert lon/lat into Point objects + set CRS\n",
    "pts = gpd.points_from_xy(\n",
    "    tokyo[\"longitude\"],\n",
    "    tokyo[\"latitude\"],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "# Convert lon/lat points to Japanese CRS in metres\n",
    "pts = gpd.GeoDataFrame({\"geometry\": pts}).to_crs(epsg=2459)\n",
    "# Extract coordinates from point objects into columns\n",
    "tokyo[\"X_metres\"] = pts.geometry.x\n",
    "tokyo[\"Y_metres\"] = pts.geometry.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to consider a cluster photos with more than 50 photos within 100 metres from them, hence we set the two parameters accordingly. Once ready, we _\"fit\"_ it to our data, but note that we first need to express the longitude and latitude of our points in metres (see code for that on the side cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(tokyo[[\"X_metres\", \"Y_metres\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, we can recover the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the list of points classified as cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only the first five values\n",
    "algo.core_sample_indices_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `labels_` object always has the same length as the number of points used to run `DBSCAN`. Each value represents the index of the cluster a point belongs to. If the point is classified as *noise*, it receives a -1. Above, we can see that the first five points are effectively not part of any cluster. To make thinks easier later on, let us turn the labels into a `Series` object that we can index in the same way as our collection of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = pd.Series(algo.labels_, index=tokyo.index)\n",
    "lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we already have the clusters, we can proceed to visualize them. There are many ways in which this can be done. We will start just by coloring points in a cluster in red and noise in grey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Assign labels to tokyo table dynamically and\n",
    "# subset points that are not part of any cluster (noise)\n",
    "noise = tokyo.assign(\n",
    "    lbls=lbls\n",
    ").query(\"lbls == -1\")\n",
    "# Plot noise in grey\n",
    "ax.scatter(\n",
    "    noise[\"X_metres\"], \n",
    "    noise[\"Y_metres\"], \n",
    "    c='grey', \n",
    "    s=5, \n",
    "    linewidth=0\n",
    ")\n",
    "# Plot all points that are not noise in red\n",
    "# NOTE how this is done through some fancy indexing, where\n",
    "#      we take the index of all points (tw) and substract from\n",
    "#      it the index of those that are noise\n",
    "ax.scatter(\n",
    "    tokyo.loc[tokyo.index.difference(noise.index), \"X_metres\"], \n",
    "    tokyo.loc[tokyo.index.difference(noise.index), \"Y_metres\"],\n",
    "    c=\"red\", \n",
    "    linewidth=0\n",
    ")\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a first good pass. The algorithm is able to identify a few clusters with high density of photos. However, this is all contingent on the parameters we arbitrarily set. Depending on the maximum radious (`eps`) we set, we will pick one type of cluster or another: a higher (lower) radious will translate in less (more) local clusters. Equally, the minimum number of points required for a cluster (`min_samples`) will affect the implicit size of the cluster. Both parameters need to be set before running the algorithm, so our decision will affect the final outcome quite significantly.\n",
    "\n",
    "For an illustration of this, let's run through a case with very different parameter values. For example, let's pick a larger radious (e.g. 500m) and a smaller number of points (e.g. 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up algorithm\n",
    "algo = DBSCAN(eps=500, min_samples=10)\n",
    "# Fit to Tokyo projected points\n",
    "algo.fit(tokyo[[\"X_metres\", \"Y_metres\"]])\n",
    "# Store labels\n",
    "lbls = pd.Series(algo.labels_, index=tokyo.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's now visualise the result (toggle the expandable to see the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Assign labels to tokyo table dynamically and\n",
    "# subset points that are not part of any cluster (noise)\n",
    "noise = tokyo.assign(\n",
    "    lbls=lbls\n",
    ").query(\"lbls == -1\")\n",
    "# Plot noise in grey\n",
    "ax.scatter(\n",
    "    noise[\"X_metres\"], \n",
    "    noise[\"Y_metres\"], \n",
    "    c='grey', \n",
    "    s=5, \n",
    "    linewidth=0\n",
    ")\n",
    "# Plot all points that are not noise in red\n",
    "# NOTE how this is done through some fancy indexing, where\n",
    "#      we take the index of all points (tw) and substract from\n",
    "#      it the index of those that are noise\n",
    "ax.scatter(\n",
    "    tokyo.loc[tokyo.index.difference(noise.index), \"X_metres\"], \n",
    "    tokyo.loc[tokyo.index.difference(noise.index), \"Y_metres\"],\n",
    "    c=\"red\", \n",
    "    linewidth=0\n",
    ")\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is now very different, isn't it? This exemplifies how different parameters can give rise to substantially different outcomes, even if the same data and algorithm are applied.\n",
    "\n",
    "### Advanced plotting\n",
    "\n",
    "As we have seen, the choice of parameters plays a crucial role in the number, shape and type of clusters founds in a dataset. To allow an easier exploration of these effects, in this section we will turn the computation and visualization of `DBSCAN` outputs into a single function. This in turn will allow us to build an interactive tool later on.\n",
    "\n",
    "Below is a function that accomplishes just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters(db, eps, min_samples):\n",
    "    '''\n",
    "    Compute and visualize DBSCAN clusters    \n",
    "    ...\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    db          : (Geo)DataFrame\n",
    "                  Table with at least columns `X` and `Y` for point coordinates\n",
    "    eps         : float\n",
    "                  Maximum radious to search for points within a cluster\n",
    "    min_samples : int\n",
    "                  Minimum number of points in a cluster\n",
    "    '''\n",
    "    algo = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    algo.fit(db[['X_metres', 'Y_metres']])\n",
    "    lbls = pd.Series(algo.labels_, index=db.index)\n",
    "\n",
    "    f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "    noise = db.loc[lbls==-1, ['X_metres', 'Y_metres']]\n",
    "    ax.scatter(noise['X_metres'], noise['Y_metres'], c='grey', s=5, linewidth=0)\n",
    "    ax.scatter(\n",
    "        db.loc[db.index.difference(noise.index), 'X_metres'],\n",
    "        db.loc[db.index.difference(noise.index), 'Y_metres'],\n",
    "        c='red', \n",
    "        linewidth=0\n",
    "    )\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes the following three arguments:\n",
    "\n",
    "1. `db`: a `(Geo)DataFrame` containing the points on which we will try to find the clusters.\n",
    "1. `eps`: a number (maybe with decimals, hence the `float` label in the documentation of the function) specifying the maximum distance to look for neighbors that will be part of a cluster.\n",
    "1. `min_samples`: a count of the minimum number of points required to form a cluster.\n",
    "\n",
    "Let us see how the function can be used. For example, let us replicate the plot above, with a minimum of 10 points and a maximum radious of 500 metres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters(tokyo, 500, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! With just one line of code, we can create a map of `DBSCAN` clusters. How cool is that? \n",
    "\n",
    "However, this could be even more interesting if we didn't have to write each time the parameters we want to explore. To change that, we can create a quick interactive tool that will allow us to modify both parameters with sliders. To do this, we will use the library [`ipywidgets`](https://ipywidgets.readthedocs.io). Let us first do it and then we will analyse it bit by bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    clusters,                 # Method to make interactive\n",
    "    db=fixed(tokyo),          # Data to pass on db (does not change)\n",
    "    eps=(50, 500, 50),        # Range start/end/step of eps\n",
    "    min_samples=(50, 300, 50) # Range start/end/step of min_samples\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That is cool, isn't it? Once passed the first excitement, let us have a look at how we built it, and how you can modify it further on. A few points on this:\n",
    "\n",
    "* First, `interact` is a method that allows us to pass an arbitrary function (like `clusters`) and turn it into an interactive widget where we modify the values of its parameters through sliders, drop-down menus, etc.\n",
    "* What we need to pass to `interact` is the name of the function we would like to make interactive (`clusters` in this case), and all the parameters it will take.\n",
    "* Since in this case we do not wish to modify the dataset that is used, we pass `tokyo` as the `db` argument in `clusters` and fixate it by passing it first to the `fixed` method.\n",
    "* Then both the radious `eps` and the minimum cluster size `min_samples` are passed. In this case, we do want to allow interactivity, so we do not use `fixed`. Instead, we pass a tuple that specifies the range and the step of the values we will allow to be used. \n",
    "* In the case of `eps`, we use `(50, 500, 50)`, which means we want `r` to go from 50 to 500, in jumps of 50 units at a time. Since these are specified in metres, we are saying we want the range to go from 50 to 500 metres in increments of 50 metres.\n",
    "* In the case of `min_samples`, we take a similar approach and say we want the minimum number of points to go from 50 to 300, in steps of 50 points at a time.\n",
    "\n",
    "The above results in a little interactive tool that allows us to play easily and quickly with different values for the parameters and to explore how they affect the final outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
